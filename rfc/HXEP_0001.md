# HXEP-0001: Retries

Applications talking to servers via HTTP over a network need to be resilient to faults and unexpected errors, such as random network faults, hosts briefly going out of service or temporarily denying requests, etc.

For this reason, HTTPX should probably have retry functionality built-in.

- How much should there be in core?
- How can we empower users to solve more advanced use cases?

[Minimum viable functionality](#minimum-viable-functionality) presents what I think can be the minimum set of dials we should expose to provide basic but meaningful retry functionality to users.

[A possible extension mechanism](#a-possible-extension-mechanism) explores a set of APIs that could allow us to defer any sufficiently advanced use cases to be implemented by users and third-party packages.

So, let's start with…

## Precedents

Requests provides retries semi-officially via urllib3's `Retry` utility class. They are **off-by-default** in Requests. (But `urllib3` uses `Retry(3)` by default, i.e. retry at most three times.)

Ignoring redirects (we already handle those separately), and based on https://github.com/encode/httpx/issues/108#issuecomment-509702260, there are 8 dials in the `Retry` helper class:

- `respect_retry_after_header`: respecting `Retry-After` headers. Applicable to 413 Payload Too Large, 429 Too Many Requests, and 503 Service Unavailable responses only. Can be turned off.
- `status_blocklist`: Retrying on specific status codes.
- `method_whitelist`: retrying on specific HTTP methods only. Defaults to idempotent methods, i.e. GET, HEAD, PUT, DELETE, OPTIONS, TRACE.
- `total`: limit the total number of errors before giving up.
- `connect`: limit the number of connection errors before giving up.
- `read`: limit the number of read errors before giving up.
- `status`: limit the number of times we receive a response whose status is in `status_blocklist` and method is in `method_whitelist` before giving up.
- `backoff_factor`: a dial for the exponential backoff algorithm used to exponentially increase the amount of time we wait before issuing a new request.

## What does a retry API involve?

If we take a step back and think about what fundamental questions a retry API should answer, IMO there are two:

1. Given an exception that occurred while trying to receive and response, should we retry and issue a new request?
2. If so, *when* should we retry, i.e. how far in the future? (i.e. sequence of retry delays)

An important note is that these two questions are completely independant — we can vary the policy for deciding whether to retry independantly from the one deciding how long to wait for before retrying.

## Minimum viable functionality

Based on this model, as a first pass I think we can get a minimum viable retry functionality with **only 1 dial**:

- `limit`: how many retryable `HTTPError` we're allowed to retry on until we give up. This dial is required so that users can turn retries on or off.

This only answers question 1), but I think question 2) can be answered without being customizable, e.g. use an exponential backoff with a reasonable factor, e.g. {0s, 0.2s, 0.4s, 0.8s, ...}.

The API would look like this…

```python
# Default: don't retry.
r = httpx.get(url)

# Retry at most 3 times.
r = httpx.get(url, retries=3)
client = httpx.Client(retries=3)
```

Under the hood this will probably end up being governed by a `Retries(limit: int = None)` config class, similar in spirit to `Timeout`. Not obvious that we need it right away as there will only be 1 dial, but will definitely be needed when we start introducing other dials.

We'll also need to define what "retryable" means. Indeed, not all exceptions can be retried on. E.g. we definitely don't want to retry on a usage error (such as `StreamConsumed`), or an error that might imply that the server has already started processing the request (such as `WriteTimeout`). (IMO this can be done by introducing a `RetryableError(HTTPError)` base class and using multiple inheritance to mark exceptions that are retryable. But that's an implementation detail at this point.)

In the end, the client would implement the actual logic for retries as a `.send_handling_retries()` method.

## What we don't need to support right away, but can probably support later

- Respecting `Retry-After` headers. (According to specs, clients *should* respect it, but they don't *have to*.)
- Customizing the delaying mechanism. (E.g. we can make the backoff factor tweakable.)
- Customizing the set of methods we can potentially retry on. (Idempotent methods are a good default, but users may need to exclude/include methods for specific use cases.)

At that point the `Retries` constructor could evolve into…

```python
Retries(
    limit=3,
    backoff_factor=0.2,
    respect_retry_after_headers=True,
)
```

… But it might not need to. (See ideas on an extension mechanism below.)

## What we probably don't need to support

These can probably be handled by the extension mechanism described in the next section...

- Fine-grained control of what type of error to retry on (e.g. `connect` vs `read`).
- Retrying on custom status codes.

## A possible extension mechanism

### Motivation

I think we probably don't want to end up having to increase the number of dials for ever to address a potentially infinite number of weird use cases users might need to address.

So an extension mechanism for retries is probably what we should think about right away. @sethmlarson mentioned in https://github.com/encode/httpx/issues/108#issuecomment-509728486:

> So I definitely want to make the new Retry object have one critical feature that actually makes sub-classing useful: Have a method that is subclassable that passes in the Request and the Response and then by some interface is allowed to decide whether a retry occurs and in addition also allows modifying the Request that will be emitted for that retry.

I don't know if it can be done with just a method, but we can definitely build generator-based sans-I/O APIs to allow users to address advanced scenarios (as we did for auth classes).

### `RetryLimiter` API

An API to answer question 1) ("Should we retry?") might look like this (not sure about the naming):

```python
class RetryLimiter:
    def retries(self, request: Request) -> Generator[Request, HTTPError, None]:
        ...
```

The default behavior, "retry at most N times", could be implemented as:

```python
class MaxErrors(RetryLimiter):
    def __init__(self, max_errors: int):
        self.max_errors = max_errors

    def retries(self, request: Request) -> Generator[Request, HTTPError, None]:
        retries_left = self.max_errors

        while True:
           # Send the request, and get any exception back...
            exc = yield request

            # Are we out of retries?
            if not retries_left:
                return

            # Modify the request if necessary, update any state, and repeat…
            request.headers["X-Foo"] = "bar"
            retries_left -= 1
```

A no-op behavior, i.e. "Never retry", would look like this:

```python
class DontRetry(RetryLimiter):
    def retries(self, request: Request) -> Generator[Request, HTTPError, None]:
        yield request
```

### `RetrySchedule` API

As for question 2 ("How far in the future should we retry?"), essentially all we need is an unbounded sequence of numbers representing the time to wait between the `N`-th and the `N+1`-th requests. It's probably safe to assume that this time is independant of the request and the response.

So the API might look like this (again, not sure about the naming):

```python
class RetrySchedule:
    def delays(self) -> Iterator[float]:
        ...
```

The most basic policy would be to wait for a constant time, e.g…

```python
class WaitFor(RetrySchedule):
    def __init__(self, seconds: float):
        self.seconds = seconds

    def delays(self) -> Iterator[float]:
        while True:
            yield self.seconds
```

Exponential backoff could also be implemented in this fashion:

```python
import itertools

class ExponentialBackoff(RetrySchedule):
    def __init__(self, backoff: float):
        self.factor = factor

    def delays(self) -> Iterator[float]:
        yield 0  # Retry immediately.
        for n in itertools.count(2):
            yield backoff_factor * (2 ** (n - 2))
```

### Impact on the `Retries` config class

Provided these two APIs exist, we could reduce the `Retries` config class down to:

```python
# This...
retries = Retries(3)

# Equivalent to...
retries = Retries(limit=3, backoff_factor=0.2)

# Which is equivalent to...
retries = Retries(limit=MaxErrors(3), schedule=ExponentialBackoff(factor=0.2))

# At this level, users could pass their own classes...
retries = Retries(
    limit=MaxErrorResponses(3, status_codes={502, 503}),
    schedule=WaitFor(1),
)
```

And so `Retries` would evolve into something like this, exposing its own generator-based method for the `Client`/`AsyncClient` to consume...

```python
import __magic__  # Utilities

from .exceptions 
RetriesTypes = typing.Union[None, int, ]

class Retries:
    def __init__(
        self,
        retries: RetriesTypes = None,
        *,
        limit: typing.Union[typing.Optional[int], RetryLimiter, UnsetType] = UNSET,
        backoff_factor: typing.Union[float, UnsetType] = UNSET,
        schedule: typing.Union[RetrySchedule, UnsetType] = UNSET,
        respect_retry_after_headers: bool = True,
    ):
        ...

    def retry_flow(
        self, request: Request,
    ) -> Generator[Union[Request, float], HTTPError, None]:
        ...
```

### Composing behavior

The last bit of functionality we might want to add to the retries API is composition of limiters, a bit like how the Django REST Framework allows to compose permissions.

This would address the issue of composing different rules or extending the default behavior, without reimplementing a bunch of code. It would allow doing things like:

```python
retries = Retries(
    limit=(
        # At most 5 errors in total, but only up to 3 error responses on these status codes...
        MaxErrors(3) | MaxErrorResponses(5, status_codes={502, 503})
    ),
)
```

But this is really far-off, we can definitely revisit if/when the use case appears.
